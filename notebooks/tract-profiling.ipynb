{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from dipy.io.streamline import load_tractogram\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.segment.metric import AveragePointwiseEuclideanMetric, ResampleFeature\n",
    "import dipy.stats.analysis as dsa\n",
    "\n",
    "import dipy.tracking.streamline as dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout(\"/scratch/knavynde/topsy/\", derivatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLURM_TMPDIR = Path(os.environ.get(\"SLURM_TMPDIR\"))\n",
    "dwi_path = Path(\"/project/6033503/cfmm-bids/Palaniyappan/TOPSY/baseline/correct7T.v0.3.1/prepdwi_v0.0.12c/work/sub-001/dwi/uncorrected_denoise_unring_topup_eddy_regT1/dwi.nii.gz\")\n",
    "t1_path = Path('/scratch/knavynde/topsy/sub-001/anat/sub-001_space-orig_desc-preproc_T1w.nii.gz')\n",
    "t1_map_path = Path('/scratch/knavynde/topsy/sub-001/anat/sub-001_acq-MP2RAGE_run-01_T1map.nii.gz')\n",
    "fa_path = Path('/project/6033503/cfmm-bids/Palaniyappan/TOPSY/baseline/correct7T.v0.3.1/prepdwi_v0.0.12c/work/sub-001/dwi/uncorrected_denoise_unring_topup_eddy_regT1/dti_FA.nii.gz')\n",
    "tracts_path = SLURM_TMPDIR/'tck-tracts'\n",
    "clusters_left = SLURM_TMPDIR/'tck-clusters/left'\n",
    "clusters_right = SLURM_TMPDIR/'tck-clusters/right'\n",
    "\n",
    "assert tracts_path.exists() and clusters_left.exists() and clusters_right.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1map = nib.load(t1_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_f_l_path = tracts_path/\"T_Sup-F_left.tck\"\n",
    "tracts = load_tractogram(str(rand_cluster), str(t1_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlines = tracts.streamlines\n",
    "tracts_actor = actor.line(streamlines, cmap.line_colors(streamlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dwi.get_fdata()[..., 0]\n",
    "mean, std = data[data > 0].mean(), data[data > 0].std()\n",
    "value_range = (mean - 0.5 * std, mean + 1.5 * std)\n",
    "slice_actor = actor.slicer(data, dwi.affine, value_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = window.Scene()\n",
    "#slice_actor.display(z=25)\n",
    "scene.add(tracts_actor)\n",
    "#scene.add(slice_actor)\n",
    "scene.reset_camera()\n",
    "scene.elevation(130)\n",
    "img = window.snapshot(scene, size=(1000,1000))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractProfile:\n",
    "    def __init__(self, streamlines, ref):\n",
    "        cluster = load_tractogram(str(streamlines), str(ref))\n",
    "        if cluster is False:\n",
    "            raise Exception(f\"Cluster {streamlines} could not be loaded\")\n",
    "\n",
    "        if not cluster:\n",
    "            self.streamlines = None\n",
    "            return\n",
    "        feature = ResampleFeature(nb_points=100)\n",
    "        metric = AveragePointwiseEuclideanMetric(feature)\n",
    "\n",
    "        qb = QuickBundles(np.inf, metric=metric)\n",
    "        cluster_bundle = qb.cluster(cluster.streamlines)\n",
    "\n",
    "        self.cluster_bundle = cluster_bundle\n",
    "        self.streamlines = cluster.streamlines\n",
    "\n",
    "        self.weights = dsa.gaussian_weights(self.streamlines)\n",
    "\n",
    "\n",
    "    def get_profile(self, img):\n",
    "        return dsa.afq_profile(\n",
    "            img.get_fdata(),\n",
    "            dts.orient_by_streamline(\n",
    "                self.streamlines,\n",
    "                self.cluster_bundle.centroids[0]\n",
    "            ),\n",
    "            img.affine,\n",
    "            weights=self.weights\n",
    "        )\n",
    "\n",
    "\n",
    "def get_profiles_and_streamlines(paths, param_maps, ref):\n",
    "    profiles = np.empty((len(param_maps), len(paths), 100))\n",
    "    streamlines = np.empty(len(paths))\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        profile = TractProfile(path, ref)\n",
    "        if profile.streamlines:\n",
    "            streamlines[i] = len([*profile.streamlines])\n",
    "            for j, param_map in enumerate(param_maps.values()):\n",
    "                profiles[j, i] = profile.get_profile(param_map)\n",
    "        else:\n",
    "            streamlines[i] = 0\n",
    "            profiles[:, i, :] = 0\n",
    "    return profiles.mean(axis=2), streamlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clusters_left\n",
    "ref_img = t1_path\n",
    "\n",
    "parameter_maps = {\n",
    "    \"FA\": nib.load(fa_path)\n",
    "}\n",
    "\n",
    "if data.is_dir():\n",
    "    paths = [*data.glob(\"*.tck\")][:25]\n",
    "else:\n",
    "    raise FileNotFoundError(\"Input must be a directory\")\n",
    "\n",
    "profiles, streamlines = get_profiles_and_streamlines(paths, parameter_maps, ref_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_numbers = (\n",
    "    re.search(r'(?<=cluster_)\\d{5}(?=\\.tck$)', str(path))[0] for path in paths\n",
    ")\n",
    "profile_table = pd.DataFrame(\n",
    "    {\n",
    "        key: data for key, data in zip(parameter_maps, profiles)\n",
    "    }\n",
    ").assign(\n",
    "    cluster=pd.Series(cluster_numbers),\n",
    "    subject=1,\n",
    "    streamlines=pd.Series(streamlines)\n",
    ").set_index([\"subject\", \"cluster\"])\n",
    "\n",
    "profile_table\n",
    "with Path(\"test.csv\").open('w') as f:\n",
    "    profile_table.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = (\n",
    "    pd.read_csv(path, index_col=0) for path in [\"test.csv\", \"test.csv\"]\n",
    ")\n",
    "\n",
    "merged = pd.concat(dataframes)\n",
    "with Path(\"all_test.csv\").open('w') as f:\n",
    "    merged.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dipy.stats.analysis as dsa\n",
    "\n",
    "weighted_cluster = dsa.gaussian_weights(oriented_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = nib.load(fa_path)\n",
    "\n",
    "profile = TractProfile(clusters_left/\"cluster_00733.tck\", t1_path).get_profile(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "axes = plt.axes()\n",
    "axes.set_ylabel(\"Fractional anisotropy\")\n",
    "axes.set_xlabel(\"Node along CST\")\n",
    "plt.plot(profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_csv(\"results/tract_profiles.csv\", index_col=(0,1))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with Path(\"resources/layer_assignments.pyc\").open('rb') as f:\n",
    "    layer_dict = pickle.load(f)\n",
    "\n",
    "with Path(\"resources/tract_assignments.pyc\").open('rb') as f:\n",
    "    tracts_dict = pickle.load(f)\n",
    "\n",
    "def get_cluster_number(name: str):\n",
    "    return int(re.search(r\"\\d{5}(?=\\.vtp)\", name)[0])\n",
    "\n",
    "hem_clusters = layer_dict[\"hemispheric\"]\n",
    "left_hem = [get_cluster_number(path) for path in hem_clusters]\n",
    "right_hem = [left + 800 for left in left_hem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    4,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    4,\n",
    "    1,\n",
    "    3,\n",
    "    4,\n",
    "    2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superficial = [\n",
    "    \"T_Sup-F\",\n",
    "    \"T_Sup-FP\",\n",
    "    \"T_Sup-O\",\n",
    "    \"T_Sup-OT\",\n",
    "    \"T_Sup-P\",\n",
    "    \"T_Sup-PO\",\n",
    "    \"T_Sup-PT\",\n",
    "    \"T_Sup-T\"\n",
    "]\n",
    "assoc = [\n",
    "    \"T_AF\",\n",
    "    \"T_CB\",\n",
    "    \"T_EC\",\n",
    "    \"T_EmC\",\n",
    "    \"T_ILF\",\n",
    "    \"T_IoFF\",\n",
    "    \"T_MdLF\",\n",
    "    \"T_PLIC\",\n",
    "    \"T_SLF-I\",\n",
    "    \"T_SLF-II\",\n",
    "    \"T_SLF-III\",\n",
    "    \"T_UF\"\n",
    "]\n",
    "\n",
    "sup_tracts = [\n",
    "        get_cluster_number(x) for x in it.chain.from_iterable(\n",
    "        [\n",
    "            tracts_dict[k] for k in superficial if k in tracts_dict\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "sup_tracts = sup_tracts + [tract + 800 for tract in sup_tracts]\n",
    "assoc_tracts = [\n",
    "    get_cluster_number(x) for x in it.chain.from_iterable(\n",
    "        {\n",
    "            k: tracts_dict[k] for k in assoc if k in tracts_dict\n",
    "        }.values()\n",
    "    )\n",
    "]\n",
    "assoc_tracts = assoc_tracts + [tract + 800 for tract in assoc_tracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio\n",
    "idx = pd.IndexSlice\n",
    "grouped = data.assign(group=lambda x: [categories[y] for y, _ in x.index] )\n",
    "assoc_data = data.loc[idx[:, assoc_tracts], :]\n",
    "\n",
    "\n",
    "\n",
    "sup_HC = grouped.loc[(grouped[\"group\"] == 1) & (grouped[\"streamlines\"] > 0)].loc[idx[:, sup_tracts], :]\n",
    "sup_FEP = grouped.loc[(grouped[\"group\"] == 2) & (grouped[\"streamlines\"] > 0)].loc[idx[:, sup_tracts], :]\n",
    "assoc_HC = grouped.loc[(grouped[\"group\"] == 1) & (grouped[\"streamlines\"] > 0)].loc[idx[:, assoc_tracts], :]\n",
    "assoc_FEP = grouped.loc[(grouped[\"group\"] == 2) & (grouped[\"streamlines\"] > 0)].loc[idx[:, assoc_tracts], :]\n",
    "\n",
    "\n",
    "ratio_HC = pd.DataFrame(index=sup_HC.groupby(level=0).mean().index).assign(\n",
    "    FA=sup_HC.groupby(level=0).mean()[\"FA\"]/assoc_HC.groupby(level=0).mean()[\"FA\"],\n",
    "    R1=sup_HC.groupby(level=0).mean()[\"R1\"]/assoc_HC.groupby(level=0).mean()[\"R1\"],\n",
    "    streamlines=sup_HC.groupby(level=0).sum()[\"streamlines\"]/assoc_HC.groupby(level=0).sum()[\"streamlines\"]\n",
    ")\n",
    "\n",
    "ratio_FEP = pd.DataFrame(index=sup_FEP.groupby(level=0).mean().index).assign(\n",
    "    FA=sup_FEP.groupby(level=0).mean()[\"FA\"]/assoc_FEP.groupby(level=0).mean()[\"FA\"],\n",
    "    R1=sup_FEP.groupby(level=0).mean()[\"R1\"]/assoc_FEP.groupby(level=0).mean()[\"R1\"],\n",
    "    streamlines=sup_FEP.groupby(level=0).sum()[\"streamlines\"]/assoc_FEP.groupby(level=0).sum()[\"streamlines\"]\n",
    ")\n",
    "\n",
    "ratio_means = pd.concat([\n",
    "    ratio_HC.assign(group=\"HC\"),\n",
    "    ratio_FEP.assign(group=\"FEP\")\n",
    "])\n",
    "\n",
    "aggregator = {\n",
    "    \"FA\": \"mean\",\n",
    "    \"R1\": \"mean\",\n",
    "    \"streamlines\": \"sum\"\n",
    "}\n",
    "\n",
    "means = pd.concat([\n",
    "    assoc_HC.groupby(level=0).agg(aggregator).assign(group=\"HC\", tracts=\"association\"),\n",
    "    assoc_FEP.groupby(level=0).agg(aggregator).assign(group=\"FEP\", tracts=\"association\"),\n",
    "    sup_HC.groupby(level=0).agg(aggregator).assign(group=\"HC\", tracts=\"superficial\"),\n",
    "    sup_FEP.groupby(level=0).agg(aggregator).assign(group=\"FEP\", tracts=\"superficial\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "layout = go.Layout(\n",
    "    margin = go.layout.Margin(l=10, r=10, b=10, t=50),\n",
    "    font_size=14\n",
    ")\n",
    "\n",
    "fig = px.box(means, color=\"group\", y=\"streamlines\", x=\"tracts\", points=\"all\", width=1000, template=\"seaborn\", title=\"Number of Streamlines\", labels={\n",
    "    \"FA\": \"FA\",\n",
    "    \"R1\": \"R1\",\n",
    "    \"streamlines\": \"# streamlines\"\n",
    "})\n",
    "fig.update_layout(layout)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
