{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import subprocess as sp\n",
    "from pathlib import Path\n",
    "from intersection.cortical_intersections import Connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids import BIDSLayout\n",
    "\n",
    "layout = BIDSLayout(\"results/prepdwi_recon\", validate=False)\n",
    "# raw_layout = BIDSLayout(\"../..\", validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import fury.io as fio\n",
    "import fury.utils as futil\n",
    "\n",
    "with open(\"resources/tract-assignments/hemispheric\") as f:\n",
    "    paths = f.read().splitlines()\n",
    "def get_sorted_bundle_sizes(file):\n",
    "    subject = file.entities[\"subject\"]\n",
    "    tmpdir = Path(os.environ[\"SLURM_TMPDIR\"])\n",
    "    clusters = tmpdir/\"clusters\"/subject\n",
    "    if not clusters.exists():\n",
    "        clusters.mkdir(parents=True)\n",
    "        with tarfile.open(file.path, 'r:gz') as tar:\n",
    "            tar.extractall(clusters)\n",
    "\n",
    "\n",
    "    bundle_sizes = {}\n",
    "    for path in paths:\n",
    "        pld = fio.load_polydata(str(clusters/\"tracts_left_hemisphere\"/path))\n",
    "        bundle_sizes[path] = len(futil.get_polydata_lines(pld))\n",
    "    return pd.DataFrame({\"bundle_size\": bundle_sizes}).reset_index().rename(columns={\"index\": \"bundle\"})\n",
    "\n",
    "# get_sorted_bundle_sizes(layout.get(subject=\"001\", desc=\"sorted\", space=\"T1w\", suffix=\"clusters\")[0])\n",
    "df = pd.concat([\n",
    "    get_sorted_bundle_sizes(path).assign(subject=path.entities[\"subject\"])\n",
    "    for path in layout.get(desc=\"sorted\", space=\"T1w\", suffix=\"clusters\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_color: str) -> tuple:\n",
    "    hex_color = hex_color.lstrip(\"#\")\n",
    "    if len(hex_color) == 3:\n",
    "        hex_color = hex_color * 2\n",
    "    return int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df.assign(category=[categories[int(x)] for x in df[\"subject\"]])\n",
    "gb = ddf.groupby([\"category\", \"bundle\"])\n",
    "ddf = pd.concat([gb.mean(), gb.std().rename(columns={\"bundle_size\": \"std\"})], axis=1)\n",
    "ddf = ddf.sort_values(\"bundle_size\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = px.line(\n",
    "    ddf,\n",
    "    y=\"bundle_size\",\n",
    "    color=\"category\",\n",
    "    width=584,\n",
    "    height=400,\n",
    "    labels={\n",
    "        \"x\": \"Nodes sorted by increasing betweeness\",\n",
    "        \"bundle_size\": \"Bundle Size (# fibers)\",\n",
    "        \"degree\": \"Degree\"\n",
    "    },\n",
    "    title=\"Bundle size distribution\",\n",
    "    # error_y=\"std\",\n",
    "    # markers=True\n",
    ")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "fig.add_traces([*it.chain.from_iterable(\n",
    "    (\n",
    "        go.Scatter(\n",
    "            x=dddf.index,\n",
    "            y=dddf[\"std\"]+dddf[\"bundle_size\"],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0),\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=dddf.index,\n",
    "            y=dddf[\"bundle_size\"]-dddf[\"std\"],\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0),\n",
    "            fill='tonexty',\n",
    "            fillcolor=f'rgba{(*hex_to_rgb(px.colors.qualitative.Plotly[i-1]), 0.3)}'\n",
    "        )\n",
    "    ) for i in range(1,5) if (dddf := ddf[ddf[\"category\"] == i]) is not None\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intersection import main\n",
    "from intersection import patch_merge\n",
    "from intersection.cortical_intersections import Mesh, Connectome\n",
    "tmpdir = Path(os.environ[\"SLURM_TMPDIR\"])\n",
    "subject = \"001\"\n",
    "mesh_path = Path(raw_layout.get(subject=subject, suffix=\"smoothwm\", extension=\".surf.gii\", hemi=\"L\")[0].path)\n",
    "intersection = main.get_intersection(mesh_path, tmpdir/\"prepdwi-recon/get_hemispheric_tracts\"/subject/\"L\", threads=8)\n",
    "mesh = Mesh(mesh_path)\n",
    "parcellation = patch_merge.get_parcellation(intersection.get_globbed_graph(2), mesh)\n",
    "connectome = Connectome(intersection, parcellation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fury.io as fio\n",
    "from intersection.patch_merge import merge_parcels\n",
    "atlas = merge_parcels(parcellation, mesh)\n",
    "fio.save_polydata(atlas, str(tmpdir/\"atlas.vtk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    1,\n",
    "    2,\n",
    "    4,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    4,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    4,\n",
    "    1,\n",
    "    3,\n",
    "    4,\n",
    "    2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betweenness(b_vals, threshold=.5):\n",
    "    vals = b_vals.values()\n",
    "    b_range = max(vals) - min(vals)\n",
    "    margin = b_range*threshold + min(vals)\n",
    "    above = dict((str(key), val) for key,val in filter(lambda val: val[1]>margin, b_vals.items()))\n",
    "\n",
    "    # b = dict(zip(it.count(), sorted(b_vals)))\n",
    "    df = pd.DataFrame({\"betweenness\": above})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import copy\n",
    "\n",
    "matrix = copy.deepcopy(connectome.matrix)\n",
    "np.fill_diagonal(matrix, 0)\n",
    "G = nx.from_numpy_matrix(matrix)\n",
    "for edge in G.edges:\n",
    "    G.edges[edge][\"distance\"] = 1/G.edges[edge][\"weight\"]\n",
    "df = betweenness(nx.edge_betweenness_centrality(G, weight=\"distance\"))\n",
    "print(df)\n",
    "px.scatter(df,  y=\"betweenness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {}\n",
    "for x in [(0, 14), (0, 21), (11, 17), (11, 20), (11, 21), (14, 42)]:\n",
    "    p = connectome.get_bundles_of_edge(x)\n",
    "    total = sum(p.values())\n",
    "    for k in p:\n",
    "        frac = p[k] / total\n",
    "        if frac > 0.1:\n",
    "            f[k] = frac\n",
    "print(f)\n",
    "print(\",\".join([Path(intersection.get_bundle_path(x)).with_suffix(\".vtp\").name for x in f.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs():\n",
    "    for bidsfile in layout.get(suffix=\"connectome\"):\n",
    "        with open(bidsfile.path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        np.fill_diagonal(data, 0)\n",
    "        G = nx.from_numpy_matrix(data)\n",
    "        yield G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betweenness(b_vals, threshold=.5):\n",
    "    vals = b_vals.values()\n",
    "    b_range = max(vals) - min(vals)\n",
    "    margin = b_range*threshold + min(vals)\n",
    "    above = dict(filter(lambda val: val[1]>margin, b_vals.items()))\n",
    "    return list(above.keys())\n",
    "    return len(above) / len(b_vals)\n",
    "\n",
    "    # b = dict(zip(it.count(), sorted(b_vals)))\n",
    "    # df = pd.DataFrame({\"betweenness\": b})\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for bidsfile in layout.get(suffix=\"connectome\"):\n",
    "    with open(bidsfile.path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    np.fill_diagonal(data, 0)\n",
    "    G = nx.from_numpy_matrix(data)\n",
    "    sub = int(bidsfile.entities['subject'])\n",
    "    cat = categories[sub]\n",
    "    rows.append({\n",
    "        \"subject\": sub,\n",
    "        \"category\": cat,\n",
    "        \"degree\":np.mean([*zip(*G.degree)][1]),\n",
    "        \"num_regions\": len(G.nodes),\n",
    "        \"transitivity\": nx.transitivity(G),\n",
    "        # \"efficiency\" nx.global_efficiency(G)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df[\"category\"] = df[\"category\"].astype(\"uint32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for bidsfile in layout.get(suffix=\"connectome\"):\n",
    "    with open(bidsfile.path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    np.fill_diagonal(data, 0)\n",
    "    G = nx.from_numpy_matrix(data)\n",
    "    sub = int(bidsfile.entities['subject'])\n",
    "    cat = categories[sub]\n",
    "    for edge in G.edges:\n",
    "        G.edges[edge][\"distance\"] = 1/G.edges[edge][\"weight\"]\n",
    "    b_vals = nx.edge_betweenness_centrality(G, weight=\"distance\")\n",
    "    for threshold in np.arange(0, 1, 0.1):\n",
    "        high_edges = betweenness(b_vals, threshold)\n",
    "        weight_above = sum([G.edges[edge][\"weight\"] for edge in high_edges])\n",
    "        total_weight = sum(list(zip(*G.edges(data=\"weight\")))[2])\n",
    "        rows.append({\n",
    "            \"sub\": sub,\n",
    "            \"category\": cat,\n",
    "            \"threshold\": threshold,    \n",
    "            \"weight_above\": weight_above\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"category\"] = df[\"category\"].astype(\"uint32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = iter(graphs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.box(df, x=\"category\", y=\"transitivity\", points=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "grouped = df.groupby([\"category\", \"threshold\"]).mean()\n",
    "px.scatter(grouped, x=grouped.index.get_level_values(\"threshold\"), y=\"weight_above\", color=grouped.index.get_level_values(\"category\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
